{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee7cec-6661-4e36-b368-4b6ccdcd8e50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install segmentation-models-pytorch albumentations scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.11 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aff01-f5c7-4973-bc6d-3dd1a682fd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from losses import ComboLoss, FocalLoss2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38748338",
   "metadata": {},
   "source": [
    "## 4 класса на изображении - плита, духовка, микроволновка и чайник."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3387c6f-7ff6-4876-9fd2-820884d7e919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 60\n",
    "\n",
    "def set_seed():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "#     torch.cuda.manual_seed(SEED)\n",
    "#     torch.cuda.manual_seed_all(SEED)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "#     os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES']= '1'\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DECODERS = smp._MODEL_ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c4c70-eb75-4c08-a1e1-341f8f18c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_side_by_side(image1, image2, title1='', title2=''):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axs[0].imshow(image1)\n",
    "    axs[0].set_title(title1)\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    axs[1].imshow(image2)\n",
    "    axs[1].set_title(title2)\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def dice_channels(prob, truth, threshold=0.5, eps = 1E-9):\n",
    "    num_imgs = prob.size(0)\n",
    "    num_channels = prob.size(1)\n",
    "    prob = (prob > threshold).float()\n",
    "    truth = (truth > 0.5).float()\n",
    "    prob = prob.view(num_imgs, num_channels, -1)\n",
    "    truth = truth.view(num_imgs, num_channels, -1)\n",
    "    intersection = (prob * truth)\n",
    "    score = (2. * intersection.sum(2) + eps) / (prob.sum(2) + truth.sum(2) + eps)\n",
    "    score[score >= 1] = 1\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "def train_epoch(loader, model, loss_function_seg, optimizer, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    for image, mask in loader:\n",
    "        x = image.to(device)\n",
    "        y = mask.to(device)\n",
    "        prediction_seg = model(x)\n",
    "        loss = loss_function_seg(prediction_seg, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        optimizer.zero_grad() \n",
    "        avg_loss += loss.item()\n",
    "    avg_loss /= len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def valid_epoch(loader, model, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for image, mask in loader:\n",
    "            x = image.to(device)\n",
    "            y = mask.to(device)\n",
    "            probs = torch.sigmoid(model(x))\n",
    "            scores.append(dice_channels(probs, y))\n",
    "    return torch.stack(scores).mean().item()\n",
    "\n",
    "\n",
    "def mask2rle(mask):\n",
    "    '''\n",
    "    mask: numpy array, 1 - pixel classified as target, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    rles = []\n",
    "    for channel in range(mask.shape[2]):\n",
    "        channel_mask = mask[:, :, channel]\n",
    "        pixels = channel_mask.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        rle_channel = ' '.join(str(x) for x in runs)\n",
    "        rles.append(rle_channel)\n",
    "    return ';'.join(rles)\n",
    " \n",
    "\n",
    "def get_submission_df(img_paths, transforms, model, device='mps'):\n",
    "    '''\n",
    "    img_paths: list of paths to test images\n",
    "    transforms: albumentation test transforms\n",
    "    model: the trained model\n",
    "    Returns submission dataframe\n",
    "    '''\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    submission_df = pd.DataFrame()\n",
    "    c = 0\n",
    "    with torch.no_grad():\n",
    "        for n, img_path in enumerate(img_paths):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            transformed_data = transforms(image=img)\n",
    "            transformed_img = transformed_data['image']\n",
    "            model_input = torch.from_numpy(transformed_img).permute(2, 0, 1).unsqueeze(0)\n",
    "            pred = model(model_input.to(device))\n",
    "            mask = torch.sigmoid(pred).squeeze().cpu() > 0.5\n",
    "            resized_mask = cv2.resize(\n",
    "                mask.numpy().transpose((1, 2, 0)).astype(int),\n",
    "                (img.shape[1], img.shape[0]),\n",
    "                interpolation=cv2.INTER_NEAREST\n",
    "            )\n",
    "            submission_df.loc[c, 'image_id'] = os.path.basename(img_path)\n",
    "            submission_df.loc[c, 'rle'] = mask2rle(resized_mask)\n",
    "            submission_df.loc[c, 'batch_number'] = c\n",
    "            c += 1\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a85a74-c31b-4cb7-834c-c3e43ecfe53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.05, hue=0.05, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Resize(500, 500),\n",
    "    A.RandomCrop(384, 384, p=0.9),\n",
    "    A.Normalize(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# train_transforms = A.Compose([\n",
    "#     A.GaussNoise(mean_range=(0.0, 0.0), \n",
    "#                  std_range=(0.01, 0.05)),\n",
    "#     A.RandomToneCurve()\n",
    "#     A.Rotate(limit=(-20, 20)),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.Resize(384, 384),\n",
    "#     A.Normalize(),\n",
    "#     A.RandomCrop()\n",
    "# ])\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(384, 384),\n",
    "    A.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7b086-72ad-413b-b3a5-350c94d23cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_paths, masks_paths, transforms):\n",
    "        self.img_paths = img_paths\n",
    "        self.masks_paths = masks_paths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.img_paths[item]\n",
    "        mask_path = self.masks_paths[item]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = np.load(mask_path)\n",
    "        \n",
    "        transformed_data = self.transforms(image=img, mask=mask)\n",
    "        img = transformed_data['image']\n",
    "        mask = transformed_data['mask']\n",
    "\n",
    "        return torch.from_numpy(img).permute(2, 0, 1), torch.from_numpy(mask).permute(2, 0, 1).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0abbd8-8aee-4a02-bb65-2d8689f3040b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = sorted(glob.glob('data/train/images/*.jpg'))[0]\n",
    "mask_path = sorted(glob.glob('data/train/masks/*.npy'))[0]\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = np.load(mask_path)\n",
    "\n",
    "transformed_data = train_transforms(image=img, mask=mask)\n",
    "transformed_img = transformed_data['image']\n",
    "transformed_mask = transformed_data['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb91a8-a96f-4842-b4e6-42fdd0fb5188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_images_side_by_side(img, np.sum(mask, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0624e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = sorted(glob.glob('data/train/images/*.jpg'))\n",
    "mask_path = sorted(glob.glob('data/train/masks/*.npy'))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98c104-1cde-4991-bebe-11e60371b908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(img_path)):\n",
    "    img = cv2.imread(img_path[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = np.load(mask_path[i])\n",
    "\n",
    "    transformed_data = train_transforms(image=img, mask=mask)\n",
    "    transformed_img = transformed_data['image']\n",
    "    transformed_mask = transformed_data['mask']\n",
    "    plot_images_side_by_side(transformed_img, np.sum(transformed_mask, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ddac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_mask.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9a148-5043-44a8-8a5c-e8c594d88718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23029a61-e08c-4862-9a79-d9fab2b3cd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_train_images = sorted(glob.glob('data/train/images/*.jpg'))\n",
    "all_train_masks = sorted(glob.glob('data/train/masks/*.npy'))\n",
    "\n",
    "train_images, valid_images, train_masks, valid_masks = train_test_split(\n",
    "    all_train_images,\n",
    "    all_train_masks,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "all_test_images = sorted(glob.glob('data/test/images/*.jpg'))\n",
    "\n",
    "len(train_images), len(train_masks), len(valid_images), len(valid_masks), len(all_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e159f7-d75a-4fe1-9574-750341bcac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(\n",
    "    train_images, \n",
    "    train_masks,\n",
    "    train_transforms\n",
    ")\n",
    "\n",
    "valid_dataset = SegmentationDataset(\n",
    "    valid_images, \n",
    "    valid_masks,\n",
    "    test_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b64e5",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ecdc30-be15-46ac-a35d-92ad7a215781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='efficientnet-b2',\n",
    "    encoder_weights='imagenet',\n",
    "    classes=4,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = ComboLoss(weights={'bce': 0.65, 'dice': 0.35}, channel_weights=[1]*4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer.param_groups[0]['initial_lr'] = optimizer.param_groups[0]['lr']\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       eta_min=5e-5,\n",
    "                                                       T_max=40,\n",
    "                                                       last_epoch=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9c68f-f5fe-47bb-b5a9-76a4dd800d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_score = 0\n",
    "optimizer.param_groups[0]['lr'] = 5e-6\n",
    "for epoch in range(40):\n",
    "    train_loss = train_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "    valid_score = valid_epoch(valid_loader, model, device)\n",
    "#     scheduler.step(train_loss)\n",
    "    if valid_score > best_score:\n",
    "        torch.save(model.state_dict(), 'best_model_unetPlusPlus_efnB2_2.pth')\n",
    "        best_score = valid_score\n",
    "    print(f'Epoch: {epoch}, train_loss: {train_loss:.4f}, valid_score: {valid_score:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcca7d-9cbb-439a-b792-f59ce0311dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model_unetPlusPlus_efnB2_2.pth'))\n",
    "\n",
    "best_score = valid_epoch(valid_loader, model, device)\n",
    "print(f'Best validation score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4546a-ec89-4dde-80ab-aa7602508c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_image_path = sorted(valid_images)[7]\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = cv2.imread(valid_image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    transformed_data = test_transforms(image=img)\n",
    "    transformed_img = transformed_data['image']\n",
    "    model_input = torch.from_numpy(transformed_img).permute(2, 0, 1).unsqueeze(0)\n",
    "    pred = model(model_input.to(device))\n",
    "    mask = torch.sigmoid(pred).squeeze().cpu() > 0.5\n",
    "    resized_mask = cv2.resize(\n",
    "        mask.numpy().transpose((1, 2, 0)).astype(int),\n",
    "        (img.shape[1], img.shape[0]),\n",
    "        interpolation=cv2.INTER_NEAREST\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67a39e-6ebe-415e-924a-eb7cf7df20e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_images_side_by_side(img, resized_mask.sum(2, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bd6df-2142-40d0-a935-0725a4fae5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(37):\n",
    "    test_image_path = sorted(all_test_images)[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = cv2.imread(test_image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        transformed_data = test_transforms(image=img)\n",
    "        transformed_img = transformed_data['image']\n",
    "        model_input = torch.from_numpy(transformed_img).permute(2, 0, 1).unsqueeze(0)\n",
    "        pred = model(model_input.to(device))\n",
    "        mask = torch.sigmoid(pred).squeeze().cpu() > 0.5\n",
    "        resized_mask = cv2.resize(\n",
    "            mask.numpy().transpose((1, 2, 0)).astype(int),\n",
    "            (img.shape[1], img.shape[0]),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "    plot_images_side_by_side(img, resized_mask.sum(2, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87aec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = get_submission_df(all_test_images, test_transforms, model)\n",
    "submission_df.to_csv(\n",
    "    'solution.csv', \n",
    "    header=['image_id', 'rle', 'batch_number'], \n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09325a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_3.11",
   "language": "python",
   "name": "kernel_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
